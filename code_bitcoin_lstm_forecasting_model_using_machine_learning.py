# -*- coding: utf-8 -*-
"""Code: Bitcoin LSTM forecasting model using Machine Learning

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mx7RbP7VIDcjCMDFvuNGux0aryFO0X5Y
"""

###Bitcoin LSTM forecasting model using Machine Learning

##Importing modules

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import datetime as datetime
import plotly.express as px

import time

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import activations
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from tensorflow.keras import optimizers
from sklearn import preprocessing
from joblib import dump, load

np.random.seed(7)

##Building input dataset for model

#Getting real-time data

#READ DATA
df = pd.read_csv('https://www.cryptodatadownload.com/cdd/Binance_BTCUSDT_d.csv', index_col=None)
df.head(3)

len(df)

df.reset_index(inplace = True)
df.head(3)

df.rename(columns=df.iloc[0], inplace = True)
df.head()

df.drop(df.index[0], inplace=True)
df.reset_index(drop=True, inplace = True)
df.head(3)

df = df.astype({"unix": float})
df = df.astype({"open": float})
df = df.astype({"high": float})
df = df.astype({"low": float})
df = df.astype({"close": float})
df = df.astype({"Volume BTC": float})
df = df.astype({"Volume USDT": float})
df = df.astype({"tradecount": float})

print(df.dtypes)
print("\n*** DF ***")
df_copy = df.copy()
df_copy.head(3)

#CLEAN DATA TIMESTAMP

def change_timestamp (ts):
    digit_count = len(str(ts))
    if digit_count == 12:
        return (datetime.datetime.utcfromtimestamp(ts)).strftime('%Y-%m-%d %H:%M:%S')
    else:
        return (datetime.datetime.utcfromtimestamp(ts/1000)).strftime('%Y-%m-%d %H:%M:%S')

df_copy['unix_count'] = df.unix.apply(lambda x: len(str(x)))        
df_copy['dt_correct'] = df.unix.apply(lambda x: change_timestamp(x))
df_copy['dt'] = pd.to_datetime(df_copy.dt_correct.values)
df_copy['week_day'] = df_copy.dt.apply(lambda x: x.weekday())
df_copy.sort_values(by=['unix'],ascending=[True],inplace=True)

df_work = df_copy[['dt','week_day','close','Volume BTC']]
df_work.head()

##Data Splitting and Data normalization

#SPLIT DATA INTO TEST AND TRAIN
X = df_work[['week_day','Volume BTC','close']]
Y = df_work[['close']]

#NORMALIZATION
f_transformer = preprocessing.MinMaxScaler((-1,1))
f_transformer = f_transformer.fit(X)

cnt_transformer = preprocessing.MinMaxScaler((-1,1))
cnt_transformer = cnt_transformer.fit(Y)

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle = False)

X_train_trans = f_transformer.transform(X_train)
X_test_trans = f_transformer.transform(X_test)

y_train_trans = cnt_transformer.transform(y_train)
y_test_trans = cnt_transformer.transform(y_test)

print("*** SHAPES")
print("X_train: %s, %s" % (X_train.shape[0],X_train.shape[1]))
print("X_test: %s, %s" % (X_test.shape[0],X_test.shape[1]))
print("y_train: %s, %s" % (y_train.shape[0],y_train.shape[1]))
print("y_test: %s, %s" % (y_test.shape[0],y_test.shape[1]))

print("\n*** MIN MAX")

print("TRAIN COST: %d, %d" % (X_train.close.min(), X_train.close.max()))
print("TEST COST: %d, %d" % (X_test.close.min(), X_test.close.max()))
print("TRAIN VOL: %d, %d" % (X_train['Volume BTC'].min(), X_train['Volume BTC'].max()))
print("TEST VOL: %d, %d" % (X_test['Volume BTC'].min(), X_test['Volume BTC'].max()))

print("\n*** MIN MAX PARAMETER")
print(f_transformer.data_min_)
print(f_transformer.data_max_)
print(cnt_transformer.data_min_)
print(cnt_transformer.data_max_)

X_train.head()

#CREATE LAGGING DATASET FOR TIME-SERIES
def create_dataset(X, y, time_steps=1):
    Xs, ys = [], []
    for i in range(len(X) - time_steps):
        v = X[i:(i + time_steps)]
        Xs.append(v)
        ys.append(y[i + time_steps])
    return np.array(Xs), np.array(ys)

time_steps = 1
# reshape to [samples, time_steps, n_features]
X_train_f, y_train_f = create_dataset(X_train_trans, y_train_trans, time_steps)
X_test_f, y_test_f = create_dataset(X_test_trans, y_test_trans, time_steps)

print("*** SHAPES")
print(X_train_f.shape, y_train_f.shape)
print(X_test_f.shape, y_test_f.shape)

##Determining best performing model architecture
##Trying 12 different model architectures with long-term Bitcoin dataset (since 2019) to check for lowest mean-squared-error:
#using 1,2,3 LSTM layers
#using tanh / relu activation function
#using dropout layer

#Creating a table to store mean squared errors
LSTM_layers = []
#we have 12 models where we alternate1, 2 and then 3 LSTM layers
for i in range(4):
  LSTM_layers.append('1 layer')
  LSTM_layers.append('2 layers')
  LSTM_layers.append('3 layers')

LSTM_layers = pd.DataFrame(LSTM_layers)

activation_function = []
#we have 6 models with tanh activation function and 6 with relu
for i in range(6):
  activation_function.append('tanh')
for i in range(6):
  activation_function.append('relu')

activation_function = pd.DataFrame(activation_function)

dropout_layer = []
#we altermate: 3 models without, 3 with, 3 without and then 3 with
for i in range(3):
  dropout_layer.append('no')
for i in range(3):
  dropout_layer.append('yes')
for i in range(3):
  dropout_layer.append('no')
for i in range(3):
  dropout_layer.append('yes')

dropout_layer = pd.DataFrame(dropout_layer)

mse_values = []

table = pd.DataFrame(None, columns=["LSTM layers", "activation function", "dropout layer"])
table['LSTM layers']=LSTM_layers
table['activation function']=activation_function
table['dropout layer']=dropout_layer

#1 LSTM layer, tanh activation function

model = keras.Sequential()
model.add(keras.Input(shape=((X_train_f.shape[1], X_train_f.shape[2]))))
model.add(layers.LSTM(300, return_sequences=False, activation = 'tanh'))
model.add(layers.BatchNormalization())
model.add(keras.layers.Dense(units=1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.summary()

hist = model.fit(X_train_f, y_train_f, batch_size = 30, epochs = 50, shuffle=False, validation_split=0.1)

loss = hist.history['loss']
val_loss = hist.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

import math

y_pred = model.predict(X_test_f) 

y_test_inv = cnt_transformer.inverse_transform(y_test_f)
y_pred_inv = cnt_transformer.inverse_transform(y_pred)
combined_array = np.concatenate((y_test_inv,y_pred_inv),axis=1)
combined_array2 = np.concatenate((X_test.iloc[time_steps:],combined_array),axis=1)

df_final = pd.DataFrame(data = combined_array, columns=["actual", "predicted"])
print("size: %d" % (len(combined_array)))
df_final.head(3)

from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error

results = model.evaluate(X_test_f, y_test_f)

mse_values.append(mean_squared_error(y_test_inv, y_pred_inv))

print("mse: %s" % (mean_squared_error(y_test_inv, y_pred_inv)))
print(results)

##PREPARING DATA FOR PLOTLY

a = np.repeat(1, len(y_test_inv))
b = np.repeat(2, len(y_pred_inv))

df1 = pd.DataFrame(data = np.concatenate((y_test_inv,(np.reshape(a, (-1, 1)))),axis=1), columns=["price","type"])
df2 = pd.DataFrame(data = np.concatenate((y_pred_inv,(np.reshape(b, (-1, 1)))),axis=1), columns=["price","type"])

frames = [df1, df2]
result = pd.concat(frames, ignore_index=False)

result["type"].replace({1: "actual", 2: "predict"}, inplace=True)
(result[result.type == "actual"]).head(10)

fig = px.line(result, x=result.index.values, y="price", color='type', title='Bitcoin Price')
fig.show()

#2 LSTM layers, tanh activation function

model = keras.Sequential()
model.add(keras.Input(shape=((X_train_f.shape[1], X_train_f.shape[2]))))
model.add(layers.LSTM(300, return_sequences=True, activation = 'tanh'))
model.add(layers.LSTM(300, return_sequences=False, activation = 'tanh'))
model.add(layers.BatchNormalization())
model.add(keras.layers.Dense(units=1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.summary()

hist = model.fit(X_train_f, y_train_f, batch_size = 30, epochs = 50, shuffle=False, validation_split=0.1)

loss = hist.history['loss']
val_loss = hist.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

import math

y_pred = model.predict(X_test_f) 

y_test_inv = cnt_transformer.inverse_transform(y_test_f)
y_pred_inv = cnt_transformer.inverse_transform(y_pred)
combined_array = np.concatenate((y_test_inv,y_pred_inv),axis=1)
combined_array2 = np.concatenate((X_test.iloc[time_steps:],combined_array),axis=1)

df_final = pd.DataFrame(data = combined_array, columns=["actual", "predicted"])
print("size: %d" % (len(combined_array)))
df_final.head(3)

print(np.shape(y_pred_inv))
print(np.shape(y_test_inv))

from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error

results = model.evaluate(X_test_f, y_test_f)

mse_values.append(mean_squared_error(y_test_inv, y_pred_inv))

print("mse: %s" % (mean_squared_error(y_test_inv, y_pred_inv)))
print(results)

##PREPARING DATA FOR PLOTLY

a = np.repeat(1, len(y_test_inv))
b = np.repeat(2, len(y_pred_inv))

df1 = pd.DataFrame(data = np.concatenate((y_test_inv,(np.reshape(a, (-1, 1)))),axis=1), columns=["price","type"])
df2 = pd.DataFrame(data = np.concatenate((y_pred_inv,(np.reshape(b, (-1, 1)))),axis=1), columns=["price","type"])

frames = [df1, df2]
result = pd.concat(frames, ignore_index=False)

result["type"].replace({1: "actual", 2: "predict"}, inplace=True)
(result[result.type == "actual"]).head(10)

fig = px.line(result, x=result.index.values, y="price", color='type', title='Bitcoin Price')
fig.show()

#3 LSTM layers, tanh activation function

model = keras.Sequential()
model.add(keras.Input(shape=((X_train_f.shape[1], X_train_f.shape[2]))))
model.add(layers.LSTM(300, return_sequences=True, activation = 'tanh'))
model.add(layers.LSTM(300, return_sequences=True, activation = 'tanh'))
model.add(layers.LSTM(300, return_sequences=False, activation = 'tanh'))
model.add(keras.layers.Dense(units=1))
model.add(layers.BatchNormalization())
model.compile(loss='mean_squared_error', optimizer='adam')
model.summary()

hist = model.fit(X_train_f, y_train_f, batch_size = 30, epochs = 50, shuffle=False, validation_split=0.1)

loss = hist.history['loss']
val_loss = hist.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

import math

y_pred = model.predict(X_test_f) 

y_test_inv = cnt_transformer.inverse_transform(y_test_f)
y_pred_inv = cnt_transformer.inverse_transform(y_pred)
combined_array = np.concatenate((y_test_inv,y_pred_inv),axis=1)
combined_array2 = np.concatenate((X_test.iloc[time_steps:],combined_array),axis=1)

df_final = pd.DataFrame(data = combined_array, columns=["actual", "predicted"])
print("size: %d" % (len(combined_array)))
df_final.head(3)

from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error

results = model.evaluate(X_test_f, y_test_f)

mse_values.append(mean_squared_error(y_test_inv, y_pred_inv))

print("mse: %s" % (mean_squared_error(y_test_inv, y_pred_inv)))
print(results)

##PREPARING DATA FOR PLOTLY

a = np.repeat(1, len(y_test_inv))
b = np.repeat(2, len(y_pred_inv))

df1 = pd.DataFrame(data = np.concatenate((y_test_inv,(np.reshape(a, (-1, 1)))),axis=1), columns=["price","type"])
df2 = pd.DataFrame(data = np.concatenate((y_pred_inv,(np.reshape(b, (-1, 1)))),axis=1), columns=["price","type"])

frames = [df1, df2]
result = pd.concat(frames, ignore_index=False)

result["type"].replace({1: "actual", 2: "predict"}, inplace=True)
(result[result.type == "actual"]).head(10)

fig = px.line(result, x=result.index.values, y="price", color='type', title='Bitcoin Price')
fig.show()

#1 LSTM layer, tanh activation function, dropout layer

model = keras.Sequential()
model.add(keras.Input(shape=((X_train_f.shape[1], X_train_f.shape[2]))))
model.add(layers.LSTM(300, return_sequences=False, activation = 'tanh'))
model.add(layers.BatchNormalization())
model.add(keras.layers.Dense(units=1))
model.add(keras.layers.Dropout(rate=0.2))
model.compile(loss='mean_squared_error', optimizer='adam')
model.summary()

hist = model.fit(X_train_f, y_train_f, batch_size = 30, epochs = 50, shuffle=False, validation_split=0.1)

loss = hist.history['loss']
val_loss = hist.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

import math

y_pred = model.predict(X_test_f) 

y_test_inv = cnt_transformer.inverse_transform(y_test_f)
y_pred_inv = cnt_transformer.inverse_transform(y_pred)
combined_array = np.concatenate((y_test_inv,y_pred_inv),axis=1)
combined_array2 = np.concatenate((X_test.iloc[time_steps:],combined_array),axis=1)

df_final = pd.DataFrame(data = combined_array, columns=["actual", "predicted"])
print("size: %d" % (len(combined_array)))
df_final.head(3)

from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error

results = model.evaluate(X_test_f, y_test_f)

mse_values.append(mean_squared_error(y_test_inv, y_pred_inv))

print("mse: %s" % (mean_squared_error(y_test_inv, y_pred_inv)))
print(results)

##PREPARING DATA FOR PLOTLY

a = np.repeat(1, len(y_test_inv))
b = np.repeat(2, len(y_pred_inv))

df1 = pd.DataFrame(data = np.concatenate((y_test_inv,(np.reshape(a, (-1, 1)))),axis=1), columns=["price","type"])
df2 = pd.DataFrame(data = np.concatenate((y_pred_inv,(np.reshape(b, (-1, 1)))),axis=1), columns=["price","type"])

frames = [df1, df2]
result = pd.concat(frames, ignore_index=False)

result["type"].replace({1: "actual", 2: "predict"}, inplace=True)
(result[result.type == "actual"]).head(10)

fig = px.line(result, x=result.index.values, y="price", color='type', title='Bitcoin Price')
fig.show()

#2 LSTM layers, tanh activation function, dropout layer

model = keras.Sequential()
model.add(keras.Input(shape=((X_train_f.shape[1], X_train_f.shape[2]))))
model.add(layers.LSTM(300, return_sequences=True, activation = 'tanh'))
model.add(layers.LSTM(300, return_sequences=False, activation = 'tanh'))
model.add(layers.BatchNormalization())
model.add(keras.layers.Dropout(rate=0.2))
model.add(keras.layers.Dense(units=1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.summary()

hist = model.fit(X_train_f, y_train_f, batch_size = 30, epochs = 50, shuffle=False, validation_split=0.1)

loss = hist.history['loss']
val_loss = hist.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

import math

y_pred = model.predict(X_test_f) 

y_test_inv = cnt_transformer.inverse_transform(y_test_f)
y_pred_inv = cnt_transformer.inverse_transform(y_pred)
combined_array = np.concatenate((y_test_inv,y_pred_inv),axis=1)
combined_array2 = np.concatenate((X_test.iloc[time_steps:],combined_array),axis=1)

df_final = pd.DataFrame(data = combined_array, columns=["actual", "predicted"])
print("size: %d" % (len(combined_array)))
df_final.head(3)

print(np.shape(y_pred_inv))
print(np.shape(y_test_inv))

from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error

results = model.evaluate(X_test_f, y_test_f)

mse_values.append(mean_squared_error(y_test_inv, y_pred_inv))

print("mse: %s" % (mean_squared_error(y_test_inv, y_pred_inv)))
print(results)

##PREPARING DATA FOR PLOTLY

a = np.repeat(1, len(y_test_inv))
b = np.repeat(2, len(y_pred_inv))

df1 = pd.DataFrame(data = np.concatenate((y_test_inv,(np.reshape(a, (-1, 1)))),axis=1), columns=["price","type"])
df2 = pd.DataFrame(data = np.concatenate((y_pred_inv,(np.reshape(b, (-1, 1)))),axis=1), columns=["price","type"])

frames = [df1, df2]
result = pd.concat(frames, ignore_index=False)

result["type"].replace({1: "actual", 2: "predict"}, inplace=True)
(result[result.type == "actual"]).head(10)

fig = px.line(result, x=result.index.values, y="price", color='type', title='Bitcoin Price')
fig.show()

#3 LSTM layers, tanh activation function, dropout layer

model = keras.Sequential()
model.add(keras.Input(shape=((X_train_f.shape[1], X_train_f.shape[2]))))
model.add(layers.LSTM(300, return_sequences=True, activation = 'tanh'))
model.add(layers.LSTM(300, return_sequences=True, activation = 'tanh'))
model.add(layers.LSTM(300, return_sequences=False, activation = 'tanh'))
model.add(keras.layers.Dropout(rate=0.2))
model.add(keras.layers.Dense(units=1))
model.add(layers.BatchNormalization())
model.compile(loss='mean_squared_error', optimizer='adam')
model.summary()

hist = model.fit(X_train_f, y_train_f, batch_size = 30, epochs = 50, shuffle=False, validation_split=0.1)

loss = hist.history['loss']
val_loss = hist.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

import math

y_pred = model.predict(X_test_f) 

y_test_inv = cnt_transformer.inverse_transform(y_test_f)
y_pred_inv = cnt_transformer.inverse_transform(y_pred)
combined_array = np.concatenate((y_test_inv,y_pred_inv),axis=1)
combined_array2 = np.concatenate((X_test.iloc[time_steps:],combined_array),axis=1)

df_final = pd.DataFrame(data = combined_array, columns=["actual", "predicted"])
print("size: %d" % (len(combined_array)))
df_final.head(3)

from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error

results = model.evaluate(X_test_f, y_test_f)

mse_values.append(mean_squared_error(y_test_inv, y_pred_inv))

print("mse: %s" % (mean_squared_error(y_test_inv, y_pred_inv)))
print(results)

##PREPARING DATA FOR PLOTLY

a = np.repeat(1, len(y_test_inv))
b = np.repeat(2, len(y_pred_inv))

df1 = pd.DataFrame(data = np.concatenate((y_test_inv,(np.reshape(a, (-1, 1)))),axis=1), columns=["price","type"])
df2 = pd.DataFrame(data = np.concatenate((y_pred_inv,(np.reshape(b, (-1, 1)))),axis=1), columns=["price","type"])

frames = [df1, df2]
result = pd.concat(frames, ignore_index=False)

result["type"].replace({1: "actual", 2: "predict"}, inplace=True)
(result[result.type == "actual"]).head(10)

fig = px.line(result, x=result.index.values, y="price", color='type', title='Bitcoin Price')
fig.show()

#1 LSTM layer, relu activation function

model = keras.Sequential()
model.add(keras.Input(shape=((X_train_f.shape[1], X_train_f.shape[2]))))
model.add(layers.LSTM(300, return_sequences=False, activation = 'tanh'))
model.add(layers.BatchNormalization())
model.add(keras.layers.Dense(units=1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.summary()

hist = model.fit(X_train_f, y_train_f, batch_size = 30, epochs = 50, shuffle=False, validation_split=0.1)

loss = hist.history['loss']
val_loss = hist.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

import math

y_pred = model.predict(X_test_f) 

y_test_inv = cnt_transformer.inverse_transform(y_test_f)
y_pred_inv = cnt_transformer.inverse_transform(y_pred)
combined_array = np.concatenate((y_test_inv,y_pred_inv),axis=1)
combined_array2 = np.concatenate((X_test.iloc[time_steps:],combined_array),axis=1)

df_final = pd.DataFrame(data = combined_array, columns=["actual", "predicted"])
print("size: %d" % (len(combined_array)))
df_final.head(3)

from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error

results = model.evaluate(X_test_f, y_test_f)

mse_values.append(mean_squared_error(y_test_inv, y_pred_inv))

print("mse: %s" % (mean_squared_error(y_test_inv, y_pred_inv)))
print(results)

##PREPARING DATA FOR PLOTLY

a = np.repeat(1, len(y_test_inv))
b = np.repeat(2, len(y_pred_inv))

df1 = pd.DataFrame(data = np.concatenate((y_test_inv,(np.reshape(a, (-1, 1)))),axis=1), columns=["price","type"])
df2 = pd.DataFrame(data = np.concatenate((y_pred_inv,(np.reshape(b, (-1, 1)))),axis=1), columns=["price","type"])

frames = [df1, df2]
result = pd.concat(frames, ignore_index=False)

result["type"].replace({1: "actual", 2: "predict"}, inplace=True)
(result[result.type == "actual"]).head(10)

fig = px.line(result, x=result.index.values, y="price", color='type', title='Bitcoin Price')
fig.show()

#2 LSTM layers, relu activation function

model = keras.Sequential()
model.add(keras.Input(shape=((X_train_f.shape[1], X_train_f.shape[2]))))
model.add(layers.LSTM(300, return_sequences=True, activation = 'tanh'))
model.add(layers.LSTM(300, return_sequences=False, activation = 'tanh'))
model.add(layers.BatchNormalization())
model.add(keras.layers.Dense(units=1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.summary()

hist = model.fit(X_train_f, y_train_f, batch_size = 30, epochs = 50, shuffle=False, validation_split=0.1)

loss = hist.history['loss']
val_loss = hist.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

import math

y_pred = model.predict(X_test_f) 

y_test_inv = cnt_transformer.inverse_transform(y_test_f)
y_pred_inv = cnt_transformer.inverse_transform(y_pred)
combined_array = np.concatenate((y_test_inv,y_pred_inv),axis=1)
combined_array2 = np.concatenate((X_test.iloc[time_steps:],combined_array),axis=1)

df_final = pd.DataFrame(data = combined_array, columns=["actual", "predicted"])
print("size: %d" % (len(combined_array)))
df_final.head(3)

print(np.shape(y_pred_inv))
print(np.shape(y_test_inv))

from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error

results = model.evaluate(X_test_f, y_test_f)

mse_values.append(mean_squared_error(y_test_inv, y_pred_inv))

print("mse: %s" % (mean_squared_error(y_test_inv, y_pred_inv)))
print(results)

##PREPARING DATA FOR PLOTLY

a = np.repeat(1, len(y_test_inv))
b = np.repeat(2, len(y_pred_inv))

df1 = pd.DataFrame(data = np.concatenate((y_test_inv,(np.reshape(a, (-1, 1)))),axis=1), columns=["price","type"])
df2 = pd.DataFrame(data = np.concatenate((y_pred_inv,(np.reshape(b, (-1, 1)))),axis=1), columns=["price","type"])

frames = [df1, df2]
result = pd.concat(frames, ignore_index=False)

result["type"].replace({1: "actual", 2: "predict"}, inplace=True)
(result[result.type == "actual"]).head(10)

fig = px.line(result, x=result.index.values, y="price", color='type', title='Bitcoin Price')
fig.show()

#3 LSTM layers, relu activation function

model = keras.Sequential()
model.add(keras.Input(shape=((X_train_f.shape[1], X_train_f.shape[2]))))
model.add(layers.LSTM(300, return_sequences=True, activation = 'tanh'))
model.add(layers.LSTM(300, return_sequences=True, activation = 'tanh'))
model.add(layers.LSTM(300, return_sequences=False, activation = 'tanh'))
model.add(keras.layers.Dense(units=1))
model.add(layers.BatchNormalization())
model.compile(loss='mean_squared_error', optimizer='adam')
model.summary()

hist = model.fit(X_train_f, y_train_f, batch_size = 30, epochs = 50, shuffle=False, validation_split=0.1)

loss = hist.history['loss']
val_loss = hist.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

import math

y_pred = model.predict(X_test_f) 

y_test_inv = cnt_transformer.inverse_transform(y_test_f)
y_pred_inv = cnt_transformer.inverse_transform(y_pred)
combined_array = np.concatenate((y_test_inv,y_pred_inv),axis=1)
combined_array2 = np.concatenate((X_test.iloc[time_steps:],combined_array),axis=1)

df_final = pd.DataFrame(data = combined_array, columns=["actual", "predicted"])
print("size: %d" % (len(combined_array)))
df_final.head(3)

from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error

results = model.evaluate(X_test_f, y_test_f)

mse_values.append(mean_squared_error(y_test_inv, y_pred_inv))

print("mse: %s" % (mean_squared_error(y_test_inv, y_pred_inv)))
print(results)

##PREPARING DATA FOR PLOTLY

a = np.repeat(1, len(y_test_inv))
b = np.repeat(2, len(y_pred_inv))

df1 = pd.DataFrame(data = np.concatenate((y_test_inv,(np.reshape(a, (-1, 1)))),axis=1), columns=["price","type"])
df2 = pd.DataFrame(data = np.concatenate((y_pred_inv,(np.reshape(b, (-1, 1)))),axis=1), columns=["price","type"])

frames = [df1, df2]
result = pd.concat(frames, ignore_index=False)

result["type"].replace({1: "actual", 2: "predict"}, inplace=True)
(result[result.type == "actual"]).head(10)

fig = px.line(result, x=result.index.values, y="price", color='type', title='Bitcoin Price')
fig.show()

#1 LSTM layer, relu activation function, dropout layer

model = keras.Sequential()
model.add(keras.Input(shape=((X_train_f.shape[1], X_train_f.shape[2]))))
model.add(layers.LSTM(300, return_sequences=False, activation = 'tanh'))
model.add(layers.BatchNormalization())
model.add(keras.layers.Dense(units=1))
model.add(keras.layers.Dense(units=1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.summary()

hist = model.fit(X_train_f, y_train_f, batch_size = 30, epochs = 50, shuffle=False, validation_split=0.1)

loss = hist.history['loss']
val_loss = hist.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

import math

y_pred = model.predict(X_test_f) 

y_test_inv = cnt_transformer.inverse_transform(y_test_f)
y_pred_inv = cnt_transformer.inverse_transform(y_pred)
combined_array = np.concatenate((y_test_inv,y_pred_inv),axis=1)
combined_array2 = np.concatenate((X_test.iloc[time_steps:],combined_array),axis=1)

df_final = pd.DataFrame(data = combined_array, columns=["actual", "predicted"])
print("size: %d" % (len(combined_array)))
df_final.head(3)

from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error

results = model.evaluate(X_test_f, y_test_f)

mse_values.append(mean_squared_error(y_test_inv, y_pred_inv))

print("mse: %s" % (mean_squared_error(y_test_inv, y_pred_inv)))
print(results)

##PREPARING DATA FOR PLOTLY

a = np.repeat(1, len(y_test_inv))
b = np.repeat(2, len(y_pred_inv))

df1 = pd.DataFrame(data = np.concatenate((y_test_inv,(np.reshape(a, (-1, 1)))),axis=1), columns=["price","type"])
df2 = pd.DataFrame(data = np.concatenate((y_pred_inv,(np.reshape(b, (-1, 1)))),axis=1), columns=["price","type"])

frames = [df1, df2]
result = pd.concat(frames, ignore_index=False)

result["type"].replace({1: "actual", 2: "predict"}, inplace=True)
(result[result.type == "actual"]).head(10)

fig = px.line(result, x=result.index.values, y="price", color='type', title='Bitcoin Price')
fig.show()

#2 LSTM layers, relu activation function, dropout layer

model = keras.Sequential()
model.add(keras.Input(shape=((X_train_f.shape[1], X_train_f.shape[2]))))
model.add(layers.LSTM(300, return_sequences=True, activation = 'tanh'))
model.add(layers.LSTM(300, return_sequences=False, activation = 'tanh'))
model.add(layers.BatchNormalization())
model.add(keras.layers.Dense(units=1))
model.add(keras.layers.Dense(units=1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.summary()

hist = model.fit(X_train_f, y_train_f, batch_size = 30, epochs = 50, shuffle=False, validation_split=0.1)

loss = hist.history['loss']
val_loss = hist.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

import math

y_pred = model.predict(X_test_f) 

y_test_inv = cnt_transformer.inverse_transform(y_test_f)
y_pred_inv = cnt_transformer.inverse_transform(y_pred)
combined_array = np.concatenate((y_test_inv,y_pred_inv),axis=1)
combined_array2 = np.concatenate((X_test.iloc[time_steps:],combined_array),axis=1)

df_final = pd.DataFrame(data = combined_array, columns=["actual", "predicted"])
print("size: %d" % (len(combined_array)))
df_final.head(3)

print(np.shape(y_pred_inv))
print(np.shape(y_test_inv))

from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error

results = model.evaluate(X_test_f, y_test_f)

mse_values.append(mean_squared_error(y_test_inv, y_pred_inv))

print("mse: %s" % (mean_squared_error(y_test_inv, y_pred_inv)))
print(results)

##PREPARING DATA FOR PLOTLY

a = np.repeat(1, len(y_test_inv))
b = np.repeat(2, len(y_pred_inv))

df1 = pd.DataFrame(data = np.concatenate((y_test_inv,(np.reshape(a, (-1, 1)))),axis=1), columns=["price","type"])
df2 = pd.DataFrame(data = np.concatenate((y_pred_inv,(np.reshape(b, (-1, 1)))),axis=1), columns=["price","type"])

frames = [df1, df2]
result = pd.concat(frames, ignore_index=False)

result["type"].replace({1: "actual", 2: "predict"}, inplace=True)
(result[result.type == "actual"]).head(10)

fig = px.line(result, x=result.index.values, y="price", color='type', title='Bitcoin Price')
fig.show()

#3 LSTM layers, relu activation function, dropout layer

model = keras.Sequential()
model.add(keras.Input(shape=((X_train_f.shape[1], X_train_f.shape[2]))))
model.add(layers.LSTM(300, return_sequences=True, activation = 'tanh'))
model.add(layers.LSTM(300, return_sequences=True, activation = 'tanh'))
model.add(layers.LSTM(300, return_sequences=False, activation = 'tanh'))
model.add(keras.layers.Dense(units=1))
model.add(keras.layers.Dense(units=1))
model.add(layers.BatchNormalization())
model.compile(loss='mean_squared_error', optimizer='adam')
model.summary()

hist = model.fit(X_train_f, y_train_f, batch_size = 30, epochs = 50, shuffle=False, validation_split=0.1)

loss = hist.history['loss']
val_loss = hist.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

import math

y_pred = model.predict(X_test_f) 

y_test_inv = cnt_transformer.inverse_transform(y_test_f)
y_pred_inv = cnt_transformer.inverse_transform(y_pred)
combined_array = np.concatenate((y_test_inv,y_pred_inv),axis=1)
combined_array2 = np.concatenate((X_test.iloc[time_steps:],combined_array),axis=1)

df_final = pd.DataFrame(data = combined_array, columns=["actual", "predicted"])
print("size: %d" % (len(combined_array)))
df_final.head(3)

from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error

results = model.evaluate(X_test_f, y_test_f)

mse_values.append(mean_squared_error(y_test_inv, y_pred_inv))

print("mse: %s" % (mean_squared_error(y_test_inv, y_pred_inv)))
print(results)

##PREPARING DATA FOR PLOTLY

a = np.repeat(1, len(y_test_inv))
b = np.repeat(2, len(y_pred_inv))

df1 = pd.DataFrame(data = np.concatenate((y_test_inv,(np.reshape(a, (-1, 1)))),axis=1), columns=["price","type"])
df2 = pd.DataFrame(data = np.concatenate((y_pred_inv,(np.reshape(b, (-1, 1)))),axis=1), columns=["price","type"])

frames = [df1, df2]
result = pd.concat(frames, ignore_index=False)

result["type"].replace({1: "actual", 2: "predict"}, inplace=True)
(result[result.type == "actual"]).head(10)

fig = px.line(result, x=result.index.values, y="price", color='type', title='Bitcoin Price')
fig.show()

#Computing mean-squared error for each model and determining minimum mean-squared error

mse_values = pd.DataFrame(data = mse_values, columns = ['mean-squared error'])
mse_values['mean-squared error'].round(2)

trials=[]

for i in range (12):
  trials.append('Trial %d' %(i+1))

trials = pd.DataFrame(trials, columns = ['Trial n°'])

mse_table = pd.concat((trials, table, mse_values), axis=1)
mse_table.set_index(['Trial n°'], drop=True, inplace=True)
mse_table['mean-squared error'].astype(float)
mse_table['mean-squared error'].describe().apply(lambda x: format(x, 'f'))
display(mse_table)

print(mse_table.loc[mse_table['mean-squared error'].idxmin()])

#now that we have determined the best performing model architecture, we will be adding a Bidirectional layer to check if it's performing better

#creating an empty list to store mean-squared errors' values
mse_bi_lstm = []

#3 LSTM layers, tanh activation function, bidirectional layer

model = keras.Sequential()
model.add(keras.Input(shape=((X_train_f.shape[1], X_train_f.shape[2]))))
model.add(layers.Bidirectional(layers.LSTM(300, activation = 'tanh', return_sequences=True)))
model.add(layers.LSTM(300, return_sequences=True, activation = 'tanh'))
model.add(layers.LSTM(300, return_sequences=True, activation = 'tanh'))
model.add(layers.LSTM(300, return_sequences=False, activation = 'tanh'))
model.add(layers.BatchNormalization())
#model.add(layers.Flatten())
model.add(keras.layers.Dense(units=1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.summary()

hist = model.fit(X_train_f, y_train_f, batch_size = 30, epochs = 50, shuffle=False, validation_split=0.1)

loss = hist.history['loss']
val_loss = hist.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

import math

y_pred = model.predict(X_test_f) 

y_test_inv = cnt_transformer.inverse_transform(y_test_f)
y_pred_inv = cnt_transformer.inverse_transform(y_pred)
combined_array = np.concatenate((y_test_inv,y_pred_inv),axis=1)
combined_array2 = np.concatenate((X_test.iloc[time_steps:],combined_array),axis=1)

df_final = pd.DataFrame(data = combined_array, columns=["actual", "predicted"])
print("size: %d" % (len(combined_array)))
df_final.head(3)

from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error

results = model.evaluate(X_test_f, y_test_f)

mse_bi_lstm.append(mean_squared_error(y_test_inv, y_pred_inv))

print("mse: %s" % (mean_squared_error(y_test_inv, y_pred_inv)))
print(results)

##PREPARING DATA FOR PLOTLY

a = np.repeat(1, len(y_test_inv))
b = np.repeat(2, len(y_pred_inv))

df1 = pd.DataFrame(data = np.concatenate((y_test_inv,(np.reshape(a, (-1, 1)))),axis=1), columns=["price","type"])
df2 = pd.DataFrame(data = np.concatenate((y_pred_inv,(np.reshape(b, (-1, 1)))),axis=1), columns=["price","type"])

frames = [df1, df2]
result = pd.concat(frames, ignore_index=False)

result["type"].replace({1: "actual", 2: "predict"}, inplace=True)
(result[result.type == "actual"]).head(10)

fig = px.line(result, x=result.index.values, y="price", color='type', title='Bitcoin Price')
fig.show()

#3 LSTM layers, tanh activation function, bidirectional layer, flatten layer

"""Bidirectional long-short term memory(bi-lstm) is the process of making any neural network o have the sequence information in both directions backwards (future to past) or forward(past to future). 

In bidirectional, our input flows in two directions, making a bi-lstm different from the regular LSTM. With the regular LSTM, we can make input flow in one direction, either backwards or forward. However, in bi-directional, we can make the input flow in both directions to preserve the future and the past information. 
"""

model = keras.Sequential()
model.add(keras.Input(shape=((X_train_f.shape[1], X_train_f.shape[2]))))
model.add(layers.Bidirectional(layers.LSTM(300, activation = 'tanh', return_sequences=True)))
model.add(layers.LSTM(300, return_sequences=True, activation = 'tanh'))
model.add(layers.LSTM(300, return_sequences=True, activation = 'tanh'))
model.add(layers.LSTM(300, return_sequences=False, activation = 'tanh'))
model.add(layers.BatchNormalization())
model.add(layers.Flatten())
model.add(keras.layers.Dense(units=1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.summary()

hist = model.fit(X_train_f, y_train_f, batch_size = 30, epochs = 50, shuffle=False, validation_split=0.1)

loss = hist.history['loss']
val_loss = hist.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

import math

y_pred = model.predict(X_test_f) 

y_test_inv = cnt_transformer.inverse_transform(y_test_f)
y_pred_inv = cnt_transformer.inverse_transform(y_pred)
combined_array = np.concatenate((y_test_inv,y_pred_inv),axis=1)
combined_array2 = np.concatenate((X_test.iloc[time_steps:],combined_array),axis=1)

df_final = pd.DataFrame(data = combined_array, columns=["actual", "predicted"])
print("size: %d" % (len(combined_array)))
df_final.head(3)

from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error

results = model.evaluate(X_test_f, y_test_f)

mse_bi_lstm.append(mean_squared_error(y_test_inv, y_pred_inv))

print("mse: %s" % (mean_squared_error(y_test_inv, y_pred_inv)))
print(results)

##PREPARING DATA FOR PLOTLY

a = np.repeat(1, len(y_test_inv))
b = np.repeat(2, len(y_pred_inv))

df1 = pd.DataFrame(data = np.concatenate((y_test_inv,(np.reshape(a, (-1, 1)))),axis=1), columns=["price","type"])
df2 = pd.DataFrame(data = np.concatenate((y_pred_inv,(np.reshape(b, (-1, 1)))),axis=1), columns=["price","type"])

frames = [df1, df2]
result = pd.concat(frames, ignore_index=False)

result["type"].replace({1: "actual", 2: "predict"}, inplace=True)
(result[result.type == "actual"]).head(10)

fig = px.line(result, x=result.index.values, y="price", color='type', title='Bitcoin Price')
fig.show()

#Computing mean-squared error for each model and determining minimum mean-squared error

mse_bi_lstm = pd.DataFrame(data = mse_bi_lstm, columns = ['mean-squared error'])
mse_bi_lstm['mean-squared error'].round(2)

models_bilstm = ['Bidirectional LSTM', 'Bidirectional LSTM with flatten layer']
table2 = pd.DataFrame(None, columns=["Model"])
table2['Model']=models_bilstm

trial = [1, 2]
trial = pd.DataFrame(trial, columns = ['Trial n°'])

mse_table2 = pd.concat((trial, table2, mse_bi_lstm), axis=1)
mse_table2.set_index(['Trial n°'], drop=True, inplace=True)
mse_table2['mean-squared error'].astype(float)
mse_table2['mean-squared error'].describe().apply(lambda x: format(x, 'f'))
display(mse_table2)

print(mse_table2.loc[mse_table2['mean-squared error'].idxmin()])

##Determining optimal parameters for model architecture

#Choosing model architecture parameters (conditional on loss)
#Using bidirectional layer + 3 LSTM layers model with flatten layer

model = keras.Sequential()
model.add(keras.Input(shape=((X_train_f.shape[1], X_train_f.shape[2]))))
model.add(layers.Bidirectional(layers.LSTM(300, activation = 'tanh', return_sequences=True)))
model.add(layers.LSTM(300, return_sequences=True, activation = 'tanh'))
model.add(layers.LSTM(300, return_sequences=True, activation = 'tanh'))
model.add(layers.LSTM(300, return_sequences=False, activation = 'tanh'))
model.add(layers.BatchNormalization())
model.add(layers.Flatten())
model.add(keras.layers.Dense(units=1))
model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])
model.summary()

#Tweaking parameters and checking tensorboard

from tensorflow.keras.callbacks import TensorBoard
from tensorflow.keras.callbacks import ModelCheckpoint

filepath = "Bitcoin_RNN_Final-{epoch:02d}-{val_loss:.3f}"  # unique file name that will include the epoch and the validation acc for that epoch
checkpoint = ModelCheckpoint("models/{}.model".format(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='max')) # saves only the best ones

#PARAMETERS TO TWEAK
epochs= 50
batch_size= 25

NAME = "%d EPOCHS and %d BATCHSIZE" %(epochs, batch_size)

tensorboard = TensorBoard(log_dir="logs/{}".format(NAME))

#tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #INFO messages are not printed

hist = model.fit(X_train_f, y_train_f, batch_size = batch_size, epochs = epochs, shuffle=False, validation_split=0.1, callbacks=[tensorboard, checkpoint])

import math

y_pred = model.predict(X_test_f) 

y_test_inv = cnt_transformer.inverse_transform(y_test_f)
y_pred_inv = cnt_transformer.inverse_transform(y_pred)
combined_array = np.concatenate((y_test_inv,y_pred_inv),axis=1)
combined_array2 = np.concatenate((X_test.iloc[time_steps:],combined_array),axis=1)

df_final = pd.DataFrame(data = combined_array, columns=["actual", "predicted"])
print("size: %d" % (len(combined_array)))
df_final.head(3)

loss = hist.history['loss']
val_loss = hist.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error

results = model.evaluate(X_test_f, y_test_f)

print("mse: %s" % (mean_squared_error(y_test_inv, y_pred_inv)))
print("test loss", results)
model.save("models/{}".format(NAME))

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir='logs/'

##PREPARING DATA FOR PLOTLY

a = np.repeat(1, len(y_test_inv))
b = np.repeat(2, len(y_pred_inv))

df1 = pd.DataFrame(data = np.concatenate((y_test_inv,(np.reshape(a, (-1, 1)))),axis=1), columns=["price","type"])
df2 = pd.DataFrame(data = np.concatenate((y_pred_inv,(np.reshape(b, (-1, 1)))),axis=1), columns=["price","type"])

frames = [df1, df2]
result = pd.concat(frames, ignore_index=False)

result["type"].replace({1: "actual", 2: "predict"}, inplace=True)
(result[result.type == "actual"]).head(10)

fig = px.line(result, x=result.index.values, y="price", color='type', title='Bitcoin Price')
fig.show()

#Model for choosing number of days for input dataset

#Model Architecture Base
model = keras.Sequential()
model.add(keras.Input(shape=((X_train_f.shape[1], X_train_f.shape[2]))))
model.add(layers.Bidirectional(layers.LSTM(300, activation = 'tanh', return_sequences=True)))
model.add(layers.LSTM(300, return_sequences=True, activation = 'tanh'))
model.add(layers.LSTM(300, return_sequences=True, activation = 'tanh'))
model.add(layers.LSTM(300, return_sequences=False, activation = 'tanh'))
model.add(layers.BatchNormalization())
model.add(layers.Flatten())
model.add(keras.layers.Dense(units=1))
model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])
model.summary()

#Trying different number of days as dataset input for model

mse_days = []
val_mse_days = []
loss_days = []
val_loss_days = []

days_number = list(range(1,197,14))

#Trying different datasets for the model
for i in days_number:
  if i == 1:
    print('Model using last day as input')
    hist = model.fit(X_train_f[:-i], y_train_f[:-i], batch_size = 30, epochs = 15, shuffle=False, validation_split=0.1)
    
    loss = hist.history['loss']
    loss_days.append(loss[14])

    val_loss = hist.history['val_loss']
    val_loss_days.append(val_loss[14])

    mean_squared_error = hist.history['mean_squared_error']
    mse_days.append(mean_squared_error[14])
    
    val_mean_squared_error = hist.history['val_mean_squared_error']
    val_mse_days.append(val_mean_squared_error[14])

    epochs = range(1, len(loss) + 1)

    #predicting future price for number of days
    y_pred = model.predict(X_test_f)

    y_pred = model.predict(X_test_f[:-i])
    y_pred_inv = cnt_transformer.inverse_transform(y_pred)
    y_test_inv = cnt_transformer.inverse_transform(y_test_f[:-i])
    
    combined_array = np.concatenate((y_test_inv,y_pred_inv),axis=1)
    
    df_final = pd.DataFrame(data = combined_array, columns=["predicted price with 1 day as input", "actual price"])
    print(df_final.head())

  else:
    print('Model using last %d days as input' %i)
    hist = model.fit(X_train_f[:-i], y_train_f[:-i], batch_size = 30, epochs = 15, shuffle=False, validation_split=0.1)
    
    loss = hist.history['loss']
    loss_days.append(loss[14])

    val_loss = hist.history['val_loss']
    val_loss_days.append(val_loss[14])

    mean_squared_error = hist.history['mean_squared_error']
    mse_days.append(mean_squared_error[14])
    
    val_mean_squared_error = hist.history['val_mean_squared_error']
    val_mse_days.append(val_mean_squared_error[14])

    epochs = range(1, len(loss) + 1)

    #predicting future price for number of days

    y_pred = model.predict(X_test_f)

    y_pred = model.predict(X_test_f[:-i])
    y_pred_inv = cnt_transformer.inverse_transform(y_pred)
    y_test_inv = cnt_transformer.inverse_transform(y_test_f[:-i])
    
    combined_array = np.concatenate((y_test_inv,y_pred_inv),axis=1)
    
    df_final = pd.DataFrame(data = combined_array, columns=[("predicted price with %d days as input" %i), "actual price"])
    print(df_final.head())

#Mean-Squared Errors conditional on number of days used for input

days = pd.DataFrame(days_number, columns = ['number of days used as input for time-series'])

loss_days = pd.DataFrame(data = loss_days, columns = ['loss'])

mse_days = pd.DataFrame(data = mse_days, columns = ['mean-squared error'])

val_loss_days = pd.DataFrame(data = val_loss_days, columns = ['validation loss'])

val_mse_days = pd.DataFrame(data = val_mse_days, columns = ['validation mean-squared error'])

mse_table3 = pd.concat((days, loss_days, mse_days, val_loss_days, val_mse_days), axis=1)
#mse_table3.set_index(['number of days used as input for time-series'], drop=True, inplace=True)
mse_table3['loss'].astype(float)
mse_table3['mean-squared error'].astype(float)
mse_table3['validation loss'].astype(float)
mse_table3['validation mean-squared error'].astype(float)
display(mse_table3)

print(mse_table3.loc[mse_table3['loss'].idxmin()])

print(mse_table3.loc[mse_table3['mean-squared error'].idxmin()])